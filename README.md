# AI-EWG: AI-Enhanced Web Generation Pipeline

**Automated video processing pipeline that transforms long-form video content into AI-enriched transcripts, interactive web pages, social media clips, and multi-platform packages.**

Process videos through transcription, AI enrichment, speaker diarization, clip generation, and social media package creation - all with organized show-based folder structure and intelligent naming.

## ğŸš€ Quick Start

### 1. **Start API Server**

```powershell
python src/cli.py --config config/pipeline.yaml api --port 8000
```

### 2. **Start Streamlit Dashboard**

```powershell
streamlit run dashboard.py
```

### 3. **Process Videos**

- Upload videos via dashboard
- Or place videos in `test_videos/newsroom/2024/`
- Videos are automatically discovered and processed

### 4. **View Results**

```
data/outputs/{show_folder}/{year}/{episode_id}/
â”œâ”€â”€ clips/              # Generated video clips
â”œâ”€â”€ html/              # Interactive web page
â””â”€â”€ meta/              # Metadata files
```

**ğŸ“š Documentation:**

- **[GETTING_STARTED.md](GETTING_STARTED.md)** - Quick start guide
- **[docs/NAMING_SYSTEM.md](docs/NAMING_SYSTEM.md)** - Episode naming and organization
- **[docs/n8n_docs/](docs/n8n_docs/)** - n8n integration guides
- **[NAMING_INTEGRATION_COMPLETE.md](NAMING_INTEGRATION_COMPLETE.md)** - Latest features

---

## ğŸ¯ Overview

AI-EWG is an intelligent video processing pipeline that automates the transformation of long-form video content (podcasts, webinars, news shows) into multiple formats optimized for different platforms and use cases.

### **Processing Pipeline:**

1. **Discovery** - Automatically find and catalog video files
2. **Transcription** - OpenAI Whisper with word-level timestamps
3. **Enrichment** - AI-powered metadata extraction (show name, host, topics, summaries)
4. **Diarization** - Speaker identification and segmentation
5. **Rendering** - Generate interactive HTML pages with synchronized video/transcript
6. **Clip Generation** - Intelligent highlight detection and video clip creation
7. **Social Packages** - Platform-specific content for YouTube, Instagram, TikTok, etc.
8. **Organization** - Automatic folder structure by show and year

## âœ¨ Key Features

### **AI-Powered Processing**

âœ… **Whisper Transcription** - State-of-the-art speech-to-text with GPU acceleration  
âœ… **AI Enrichment** - Automatic show name, host, episode number extraction  
âœ… **Speaker Diarization** - Identify and label different speakers  
âœ… **Topic Segmentation** - Detect topic boundaries and key moments  
âœ… **Intelligent Clipping** - AI-powered highlight detection and ranking

### **Organization & Naming**

âœ… **Show-Based Folders** - Organized by show name and year  
âœ… **Smart Episode IDs** - Format: `{show}_ep{number}_{date}`  
âœ… **Automatic Mapping** - AI show names mapped to consistent folders  
âœ… **Configurable Templates** - Customize naming via YAML config  
âœ… **Backward Compatible** - Supports both old and new structures

### **Multi-Platform Output**

âœ… **Interactive HTML** - Synchronized video player with clickable transcript  
âœ… **Video Clips** - Multiple aspect ratios (16:9, 9:16, 1:1)  
âœ… **Social Packages** - Platform-specific metadata and formatting  
âœ… **Subtitles** - VTT, SRT formats with speaker labels  
âœ… **JSON-LD** - SEO-optimized structured data

## ğŸ­ï¸ Architecture

### **Core Components**

```
src/core/
â”œâ”€â”€ pipeline.py                  # Main orchestrator
â”œâ”€â”€ naming_service.py            # Episode naming and organization
â”œâ”€â”€ registry.py                  # Episode database management
â”œâ”€â”€ discovery_engine.py          # Video file discovery
â”œâ”€â”€ clip_discovery.py            # Intelligent clip detection
â”œâ”€â”€ clip_export.py               # Video clip rendering
â”œâ”€â”€ package_generator.py         # Social media packages
â”œâ”€â”€ policy_engine.py             # Platform-specific policies
â”œâ”€â”€ job_queue.py                 # Background job processing
â””â”€â”€ models.py                    # Data models

src/stages/
â”œâ”€â”€ prep_stage.py                # Media validation
â”œâ”€â”€ transcription_stage.py       # Whisper transcription
â”œâ”€â”€ enrichment_stage.py          # AI enrichment
â””â”€â”€ rendering_stage.py           # HTML generation

src/api/
â”œâ”€â”€ server.py                    # FastAPI server
â”œâ”€â”€ endpoints.py                 # Episode endpoints
â”œâ”€â”€ clip_endpoints.py            # Clip endpoints
â””â”€â”€ social_endpoints.py          # Social package endpoints
```

### **Configuration Structure**

```
config/
â”œâ”€â”€ pipeline.yaml                # Main pipeline configuration
â””â”€â”€ platforms/                   # Social media platform configs
    â”œâ”€â”€ youtube.yaml
    â”œâ”€â”€ instagram.yaml
    â”œâ”€â”€ tiktok.yaml
    â”œâ”€â”€ x.yaml
    â””â”€â”€ facebook.yaml
```

### **Data Flow**

```
Video Discovery â†’ Prep â†’ Transcription â†’ Enrichment â†’ Rendering
                                      â†“
                              Episode ID Regeneration
                                      â†“
                    Organized Folder Structure
                    ({show_folder}/{year}/{episode_id}/)
                                      â†“
              Clip Generation â†’ Social Packages â†’ n8n Automation
```

## ğŸ› ï¸ Installation & Setup

### **Prerequisites**

- Python 3.11+
- CUDA-capable GPU (optional, for GPU acceleration)
- FFmpeg (for video processing)
- Ollama (for AI enrichment)

### **Basic Setup**

```powershell
# 1. Clone repository
git clone <repository-url>
cd ai-ewg

# 2. Create virtual environment
python -m venv venv
.\venv\Scripts\Activate.ps1

# 3. Install dependencies
pip install -r requirements.txt
python -m spacy download en_core_web_lg

# 4. Start Ollama (in separate terminal)
ollama serve
ollama pull llama3.1:latest

# 5. Start API server
python src/cli.py --config config/pipeline.yaml api --port 8000

# 6. Start dashboard (in separate terminal)
streamlit run dashboard.py
```

### **Configuration**

Edit `config/pipeline.yaml`:

```yaml
organization:
  folder_structure: "{show_folder}/{year}"
  episode_template: "{show_folder}_ep{episode_number}_{date}"
  
models:
  whisper: "large-v3"
  llm: "llama3.1:latest"
  
sources:
  - path: "test_videos/newsroom/2024"
    enabled: true
```

## ğŸ“‹ Usage Examples

### **Process via API**

```python
import requests

# Discover videos
response = requests.post("http://localhost:8000/episodes/discover")
episodes = response.json()["episodes"]

# Process episode
episode_id = episodes[0]["episode_id"]
response = requests.post(
    f"http://localhost:8000/async/episodes/{episode_id}/process",
    json={"target_stage": "rendered"}
)
job_id = response.json()["job_id"]

# Check status
response = requests.get(f"http://localhost:8000/async/jobs/{job_id}")
print(f"Status: {response.json()['status']}")
```

### **Generate Clips**

```python
# Discover clips
response = requests.post(
    f"http://localhost:8000/episodes/{episode_id}/discover_clips",
    json={"min_score": 0.15}
)

# Render clips
response = requests.post(
    f"http://localhost:8000/async/episodes/{episode_id}/render_clips",
    json={
        "variants": ["clean", "subtitled"],
        "aspect_ratios": ["16x9", "9x16"]
    }
)
```

### **Create Social Packages**

```python
# Generate social media packages
response = requests.post(
    "http://localhost:8000/social/generate",
    json={
        "episode_id": episode_id,
        "platforms": ["youtube", "instagram", "tiktok"]
    }
)
job_id = response.json()["job_id"]

# Monitor progress
response = requests.get(f"http://localhost:8000/social/jobs/{job_id}")
print(f"Packages: {response.json()['packages_generated']}")
```

## ğŸ“ Episode Naming & Organization

### **Naming Format**

Episodes are automatically organized by show and year:

```
{show_folder}_ep{episode_number}_{date}
```

**Examples:**
- `ForumDailyNews_ep140_2024-10-27`
- `BoomAndBust_ep580_2024-10-27`
- `CanadianJustice_ep335_2024-10-27`

### **Folder Structure**

```
data/outputs/
â”œâ”€â”€ ForumDailyNews/
â”‚   â””â”€â”€ 2024/
â”‚       â””â”€â”€ ForumDailyNews_ep140_2024-10-27/
â”‚           â”œâ”€â”€ clips/
â”‚           â”œâ”€â”€ html/
â”‚           â””â”€â”€ meta/
â”œâ”€â”€ BoomAndBust/
â”‚   â””â”€â”€ 2024/
â””â”€â”€ CanadianJustice/
    â””â”€â”€ 2024/
```

### **Show Mappings**

AI-extracted show names are automatically mapped to consistent folder names:

| AI Extracted | Folder Name |
|--------------|-------------|
| "Forum Daily News" | `ForumDailyNews` |
| "Boom and Bust" | `BoomAndBust` |
| "Canadian Justice" | `CanadianJustice` |
| "The LeDrew Show" | `TheLeDrewShow` |

See [docs/NAMING_SYSTEM.md](docs/NAMING_SYSTEM.md) for complete documentation.

## ğŸ§ª Testing

### **Test Naming Service**

```powershell
python test_naming_service.py
```

### **Clear All Data** (Fresh Start)

```powershell
.\clear_all_data.ps1
```

## ğŸ“Š Monitoring & Analytics

### **Built-in Metrics**

- Workflow execution times and success rates
- Content generation statistics (pages, feeds, social packages)
- Platform integration success/failure rates
- System performance metrics (CPU, memory, processing time)
- Error classification and recovery statistics

### **Workflow Reporting**

```python
# Get detailed workflow report
report = platform.get_workflow_report("workflow_20241225_143022_abc123")

print(f"Episodes processed: {report.metrics.total_episodes}")
print(f"Pages generated: {report.metrics.pages_generated}")
print(f"Social packages: {report.metrics.social_packages_generated}")
print(f"Processing time: {report.metrics.total_processing_time}")
```

## ğŸ”§ Configuration

### **Environment Variables**

```bash
# Basic settings
PUBLISHING_ENVIRONMENT=production
PUBLISHING_BASE_URL=https://example.com

# External service credentials
PUBLISHING_GOOGLE_SEARCH_CONSOLE_KEY=/path/to/key.json
PUBLISHING_BING_WEBMASTER_KEY=your_api_key
PUBLISHING_YOUTUBE_API_KEY=your_youtube_key
PUBLISHING_CLOUDFLARE_API_KEY=your_cloudflare_token

# Feature flags
PUBLISHING_SOCIAL_GENERATION=true
PUBLISHING_PLATFORM_INTEGRATION=true
PUBLISHING_CDN_MANAGEMENT=true
```

### **Configuration Files**

- **publishing.yaml** - Main platform settings
- **integrations.yaml** - External service configurations
- **environments/\*.yaml** - Environment-specific overrides
- **social_profiles.yaml** - Social media platform specifications

## ğŸš¨ Error Handling & Recovery

The platform includes comprehensive error handling with automatic recovery:

- **Transient Errors** - Automatic retry with exponential backoff
- **Configuration Errors** - Detailed validation with suggestions
- **API Errors** - Graceful degradation and alternative workflows
- **Deployment Errors** - Automatic rollback capabilities
- **Social Media Errors** - Queue management with retry logic

## ğŸ“ˆ Performance Optimization

### **Built-in Optimizations**

- Batch processing with configurable concurrency
- Intelligent caching at multiple levels
- CDN integration with cache warming
- Lazy loading of optional components
- Memory-efficient streaming for large datasets

### **Monitoring**

- Real-time performance metrics
- Resource usage tracking
- Bottleneck identification
- Optimization recommendations

## ğŸ”’ Security & Compliance

- **Secret Management** - Environment variables, encrypted files, external vaults
- **API Security** - Token-based authentication, rate limiting
- **Content Validation** - Schema compliance, link integrity, rights management
- **Access Control** - Environment-based permissions
- **Audit Logging** - Comprehensive activity tracking

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Run tests: `python test_dry_run.py`
4. Commit changes: `git commit -m 'Add amazing feature'`
5. Push to branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Documentation**: [docs/DRY_RUN_SETUP_CHECKLIST.md](docs/DRY_RUN_SETUP_CHECKLIST.md)
- **Examples**: [examples/](examples/)
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

---

## ğŸ‰ Success Stories

> "Reduced our content publishing time from 4 hours to 15 minutes with full automation and social media integration." - Content Team

> "The validation gates caught 95% of issues before production, saving us countless hours of troubleshooting." - DevOps Team

> "Social media engagement increased 300% with automated, platform-optimized content packages." - Marketing Team
