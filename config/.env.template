# Video Processing Pipeline Environment Variables
# Copy this file to .env and fill in your values

# ========================================
# API Keys & Tokens
# ========================================

# HuggingFace token for speaker diarization models
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=

# OpenAI API key (if using OpenAI for enrichment instead of Ollama)
# OPENAI_API_KEY=

# ========================================
# Paths & Directories
# ========================================

# Main newsroom video directory (optional override)
# NEWSROOM_PATH=D:/newsroom/videos

# Database path (optional override)
# DATABASE_PATH=data/pipeline.db

# Staging directory (optional override)
# STAGING_PATH=staging

# Log directory (optional override)
# LOG_DIRECTORY=logs

# ========================================
# Ollama Configuration
# ========================================

# Ollama server URL
OLLAMA_URL=http://localhost:11434

# Ollama model to use for enrichment
OLLAMA_MODEL=llama3.1:latest

# ========================================
# Model Configuration
# ========================================

# Whisper model size: tiny, base, small, medium, large, large-v2, large-v3
# Recommendation: large-v3 for best quality with RTX 4080
WHISPER_MODEL=large-v3

# Device for Whisper: auto, cuda, cpu
WHISPER_DEVICE=auto

# Compute type: int8, int8_float16, float16, float32
# Recommendation: float16 for RTX 4080
WHISPER_COMPUTE_TYPE=float16

# Speaker diarization device: cuda, cpu
DIARIZE_DEVICE=cuda

# Expected number of speakers (2 = host + guest)
DIARIZE_NUM_SPEAKERS=2

# ========================================
# Processing Limits
# ========================================

# Maximum concurrent episodes to process
# Recommendation: 2-4 for RTX 4080 depending on model size
MAX_CONCURRENT_EPISODES=4

# Maximum GPU transcriptions at once (prevents OOM)
MAX_GPU_CONCURRENT=1

# ========================================
# Confidence Thresholds
# ========================================

# Minimum confidence for entity extraction (0.0-1.0)
MIN_ENTITY_CONFIDENCE=0.7

# Minimum score for expert badge (0.0-1.0)
MIN_EXPERT_SCORE=0.75

# Minimum score to publish episode (0.0-1.0)
MIN_PUBLISH_SCORE=0.6

# ========================================
# API & Rate Limiting
# ========================================

# Delay between API calls (seconds)
API_RATE_LIMIT_DELAY=0.5

# Wikidata API rate limit (requests per second)
# WIKIDATA_RATE_LIMIT=1.0

# ========================================
# Logging
# ========================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable structured JSON logging
STRUCTURED_LOGGING=true

# ========================================
# Development & Testing
# ========================================

# Enable debug mode
# DEBUG=false

# Dry run mode (no actual processing)
# DRY_RUN=false
